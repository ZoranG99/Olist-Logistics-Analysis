{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e6e3f1b",
   "metadata": {},
   "source": [
    "### INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86851fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360caf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä DATASET: CUSTOMERS\n",
      "============================================================\n",
      "‚Ä¢ Rows: 99441 | Columns: 5\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚úÖ No missing values found.\n",
      "\n",
      "Types:\n",
      "customer_id                 object\n",
      "customer_unique_id          object\n",
      "customer_zip_code_prefix     int64\n",
      "customer_city               object\n",
      "customer_state              object\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "                        customer_id                customer_unique_id  \\\n",
      "0  06b8999e2fba1a1fbc88172c00ba8bc7  861eff4711a542e4b93843c6dd7febb0   \n",
      "1  18955e83d337fd6b2def6b18a428ac77  290c77bc529b7ac935b93aa66c333dc3   \n",
      "2  4e7b3e00288586ebd08712fdd0374a03  060e732b5b29e8181a18229c7b0b2b5e   \n",
      "\n",
      "   customer_zip_code_prefix          customer_city customer_state  \n",
      "0                     14409                 franca             SP  \n",
      "1                      9790  sao bernardo do campo             SP  \n",
      "2                      1151              sao paulo             SP  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: GEOLOCATION\n",
      "============================================================\n",
      "‚Ä¢ Rows: 1000163 | Columns: 5\n",
      "‚Ä¢ Duplicates: 261831\n",
      "\n",
      "‚úÖ No missing values found.\n",
      "\n",
      "Types:\n",
      "geolocation_zip_code_prefix      int64\n",
      "geolocation_lat                float64\n",
      "geolocation_lng                float64\n",
      "geolocation_city                object\n",
      "geolocation_state               object\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "   geolocation_zip_code_prefix  geolocation_lat  geolocation_lng  \\\n",
      "0                         1037       -23.545621       -46.639292   \n",
      "1                         1046       -23.546081       -46.644820   \n",
      "2                         1046       -23.546129       -46.642951   \n",
      "\n",
      "  geolocation_city geolocation_state  \n",
      "0        sao paulo                SP  \n",
      "1        sao paulo                SP  \n",
      "2        sao paulo                SP  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: ORDERS\n",
      "============================================================\n",
      "‚Ä¢ Rows: 99441 | Columns: 8\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚ö†Ô∏è  MISSING VALUES:\n",
      "order_approved_at                 160\n",
      "order_delivered_carrier_date     1783\n",
      "order_delivered_customer_date    2965\n",
      "dtype: int64\n",
      "\n",
      "Types:\n",
      "order_id                         object\n",
      "customer_id                      object\n",
      "order_status                     object\n",
      "order_purchase_timestamp         object\n",
      "order_approved_at                object\n",
      "order_delivered_carrier_date     object\n",
      "order_delivered_customer_date    object\n",
      "order_estimated_delivery_date    object\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "                           order_id                       customer_id  \\\n",
      "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
      "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
      "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
      "\n",
      "  order_status order_purchase_timestamp    order_approved_at  \\\n",
      "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
      "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
      "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
      "\n",
      "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
      "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
      "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
      "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
      "\n",
      "  order_estimated_delivery_date  \n",
      "0           2017-10-18 00:00:00  \n",
      "1           2018-08-13 00:00:00  \n",
      "2           2018-09-04 00:00:00  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: ORDER_ITEMS\n",
      "============================================================\n",
      "‚Ä¢ Rows: 112650 | Columns: 7\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚úÖ No missing values found.\n",
      "\n",
      "Types:\n",
      "order_id                object\n",
      "order_item_id            int64\n",
      "product_id              object\n",
      "seller_id               object\n",
      "shipping_limit_date     object\n",
      "price                  float64\n",
      "freight_value          float64\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "                           order_id  order_item_id  \\\n",
      "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
      "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
      "2  000229ec398224ef6ca0657da4fc703e              1   \n",
      "\n",
      "                         product_id                         seller_id  \\\n",
      "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
      "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
      "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
      "\n",
      "   shipping_limit_date  price  freight_value  \n",
      "0  2017-09-19 09:45:35   58.9          13.29  \n",
      "1  2017-05-03 11:05:13  239.9          19.93  \n",
      "2  2018-01-18 14:48:30  199.0          17.87  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: ORDER_PAYMENTS\n",
      "============================================================\n",
      "‚Ä¢ Rows: 103886 | Columns: 5\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚úÖ No missing values found.\n",
      "\n",
      "Types:\n",
      "order_id                 object\n",
      "payment_sequential        int64\n",
      "payment_type             object\n",
      "payment_installments      int64\n",
      "payment_value           float64\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "                           order_id  payment_sequential payment_type  \\\n",
      "0  b81ef226f3fe1789b1e8b2acac839d17                   1  credit_card   \n",
      "1  a9810da82917af2d9aefd1278f1dcfa0                   1  credit_card   \n",
      "2  25e8ea4e93396b6fa0d3dd708e76c1bd                   1  credit_card   \n",
      "\n",
      "   payment_installments  payment_value  \n",
      "0                     8          99.33  \n",
      "1                     1          24.39  \n",
      "2                     1          65.71  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: ORDER_REVIEWS\n",
      "============================================================\n",
      "‚Ä¢ Rows: 99224 | Columns: 7\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚ö†Ô∏è  MISSING VALUES:\n",
      "review_comment_title      87656\n",
      "review_comment_message    58247\n",
      "dtype: int64\n",
      "\n",
      "Types:\n",
      "review_id                  object\n",
      "order_id                   object\n",
      "review_score                int64\n",
      "review_comment_title       object\n",
      "review_comment_message     object\n",
      "review_creation_date       object\n",
      "review_answer_timestamp    object\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "                          review_id                          order_id  \\\n",
      "0  7bc2406110b926393aa56f80a40eba40  73fc7af87114b39712e6da79b0a377eb   \n",
      "1  80e641a11e56f04c1ad469d5645fdfde  a548910a1c6147796b98fdf73dbeba33   \n",
      "2  228ce5500dc1d8e020d8d1322874b6f0  f9e4b658b201a9f2ecdecbb34bed034b   \n",
      "\n",
      "   review_score review_comment_title review_comment_message  \\\n",
      "0             4                  NaN                    NaN   \n",
      "1             5                  NaN                    NaN   \n",
      "2             5                  NaN                    NaN   \n",
      "\n",
      "  review_creation_date review_answer_timestamp  \n",
      "0  2018-01-18 00:00:00     2018-01-18 21:46:59  \n",
      "1  2018-03-10 00:00:00     2018-03-11 03:05:13  \n",
      "2  2018-02-17 00:00:00     2018-02-18 14:36:24  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: PRODUCTS\n",
      "============================================================\n",
      "‚Ä¢ Rows: 32951 | Columns: 9\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚ö†Ô∏è  MISSING VALUES:\n",
      "product_category_name         610\n",
      "product_name_lenght           610\n",
      "product_description_lenght    610\n",
      "product_photos_qty            610\n",
      "product_weight_g                2\n",
      "product_length_cm               2\n",
      "product_height_cm               2\n",
      "product_width_cm                2\n",
      "dtype: int64\n",
      "\n",
      "Types:\n",
      "product_id                     object\n",
      "product_category_name          object\n",
      "product_name_lenght           float64\n",
      "product_description_lenght    float64\n",
      "product_photos_qty            float64\n",
      "product_weight_g              float64\n",
      "product_length_cm             float64\n",
      "product_height_cm             float64\n",
      "product_width_cm              float64\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "                         product_id product_category_name  \\\n",
      "0  1e9e8ef04dbcff4541ed26657ea517e5            perfumaria   \n",
      "1  3aa071139cb16b67ca9e5dea641aaa2f                 artes   \n",
      "2  96bd76ec8810374ed1b65e291975717f         esporte_lazer   \n",
      "\n",
      "   product_name_lenght  product_description_lenght  product_photos_qty  \\\n",
      "0                 40.0                       287.0                 1.0   \n",
      "1                 44.0                       276.0                 1.0   \n",
      "2                 46.0                       250.0                 1.0   \n",
      "\n",
      "   product_weight_g  product_length_cm  product_height_cm  product_width_cm  \n",
      "0             225.0               16.0               10.0              14.0  \n",
      "1            1000.0               30.0               18.0              20.0  \n",
      "2             154.0               18.0                9.0              15.0  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: SELLERS\n",
      "============================================================\n",
      "‚Ä¢ Rows: 3095 | Columns: 4\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚úÖ No missing values found.\n",
      "\n",
      "Types:\n",
      "seller_id                 object\n",
      "seller_zip_code_prefix     int64\n",
      "seller_city               object\n",
      "seller_state              object\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "                          seller_id  seller_zip_code_prefix     seller_city  \\\n",
      "0  3442f8959a84dea7ee197c632cb2df15                   13023        campinas   \n",
      "1  d1b65fc7debc3361ea86b5f14c68d2e2                   13844      mogi guacu   \n",
      "2  ce3ad9de960102d0677a81f5d0bb7b2d                   20031  rio de janeiro   \n",
      "\n",
      "  seller_state  \n",
      "0           SP  \n",
      "1           SP  \n",
      "2           RJ  \n",
      "\n",
      "\n",
      "============================================================\n",
      "üìä DATASET: CATEGORY_TRANSLATION\n",
      "============================================================\n",
      "‚Ä¢ Rows: 71 | Columns: 2\n",
      "‚Ä¢ Duplicates: 0\n",
      "\n",
      "‚úÖ No missing values found.\n",
      "\n",
      "Types:\n",
      "product_category_name            object\n",
      "product_category_name_english    object\n",
      "dtype: object\n",
      "\n",
      "üëÄ Head (First 3 rows):\n",
      "    product_category_name product_category_name_english\n",
      "0            beleza_saude                 health_beauty\n",
      "1  informatica_acessorios         computers_accessories\n",
      "2              automotivo                          auto\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup File Paths (Using raw strings r'' for Windows paths)\n",
    "files = {\n",
    "    \"customers\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_customers_dataset.csv\",\n",
    "    \"geolocation\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_geolocation_dataset.csv\",\n",
    "    \"orders\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_orders_dataset.csv\",\n",
    "    \"order_items\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_order_items_dataset.csv\",\n",
    "    \"order_payments\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_order_payments_dataset.csv\",\n",
    "    \"order_reviews\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_order_reviews_dataset.csv\",\n",
    "    \"products\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_products_dataset.csv\",\n",
    "    \"sellers\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\olist_sellers_dataset.csv\",\n",
    "    \"category_translation\": r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\raw\\product_category_name_translation.csv\"\n",
    "}\n",
    "\n",
    "def inspect_dataset(name, file_path):\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìä DATASET: {name.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå ERROR: File not found at {file_path}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # 1. Shape & Duplicates\n",
    "        print(f\"‚Ä¢ Rows: {df.shape[0]} | Columns: {df.shape[1]}\")\n",
    "        print(f\"‚Ä¢ Duplicates: {df.duplicated().sum()}\")\n",
    "        \n",
    "        # 2. Missing Values (Only show columns that actually have missing data)\n",
    "        missing = df.isnull().sum()\n",
    "        missing = missing[missing > 0]\n",
    "        if not missing.empty:\n",
    "            print(\"\\n‚ö†Ô∏è  MISSING VALUES:\")\n",
    "            print(missing)\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No missing values found.\")\n",
    "\n",
    "        # 3. Data Types (Crucial for SQL/Power BI prep)\n",
    "        print(\"\\nTypes:\")\n",
    "        print(df.dtypes)\n",
    "        \n",
    "        # 4. Preview\n",
    "        print(\"\\nüëÄ Head (First 3 rows):\")\n",
    "        print(df.head(3))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR reading file: {e}\")\n",
    "\n",
    "# Run the inspection loop\n",
    "for name, path in files.items():\n",
    "    inspect_dataset(name, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2c41b",
   "metadata": {},
   "source": [
    "### CLEANING & TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3605da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: customers\n",
      "Loaded: geolocation\n",
      "Loaded: orders\n",
      "Loaded: order_items\n",
      "Loaded: order_payments\n",
      "Loaded: order_reviews\n",
      "Loaded: products\n",
      "Loaded: sellers\n",
      "Loaded: category_translation\n",
      "------------------------------\n",
      "Processing Dates...\n",
      "‚úÖ Dates converted successfully.\n",
      "Processing Geolocation...\n",
      "‚úÖ Geolocation duplicates removed. New Shape: (19015, 5)\n",
      "Processing Products...\n",
      "‚úÖ Products translated to English.\n",
      "==============================\n",
      "Orders Date Type Check: datetime64[ns]\n",
      "Geolocation Unique Zips: 19015\n",
      "Products with English Category: 32951 / 32951\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Dataframes into memory (since the previous step only printed them)\n",
    "dfs = {}\n",
    "for name, path in files.items():\n",
    "    dfs[name] = pd.read_csv(path)\n",
    "    print(f\"Loaded: {name}\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TRANSFORM 1: Fix Date Types\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing Dates...\")\n",
    "date_cols = [\n",
    "    'order_purchase_timestamp', \n",
    "    'order_approved_at', \n",
    "    'order_delivered_carrier_date', \n",
    "    'order_delivered_customer_date', \n",
    "    'order_estimated_delivery_date'\n",
    "]\n",
    "\n",
    "# Convert to datetime (errors='coerce' turns bad data into NaT/Null instead of crashing)\n",
    "for col in date_cols:\n",
    "    dfs['orders'][col] = pd.to_datetime(dfs['orders'][col], errors='coerce')\n",
    "\n",
    "# Do the same for order_items and reviews\n",
    "dfs['order_items']['shipping_limit_date'] = pd.to_datetime(dfs['order_items']['shipping_limit_date'], errors='coerce')\n",
    "dfs['order_reviews']['review_creation_date'] = pd.to_datetime(dfs['order_reviews']['review_creation_date'], errors='coerce')\n",
    "dfs['order_reviews']['review_answer_timestamp'] = pd.to_datetime(dfs['order_reviews']['review_answer_timestamp'], errors='coerce')\n",
    "\n",
    "print(\"‚úÖ Dates converted successfully.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TRANSFORM 2: Fix Geolocation Duplicates\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing Geolocation...\")\n",
    "# We group by Zip Code and take the first Lat/Lng found. \n",
    "# This reduces rows from 1M+ to just unique Zip Codes (~19k).\n",
    "geo_clean = dfs['geolocation'].groupby('geolocation_zip_code_prefix').first().reset_index()\n",
    "dfs['geolocation'] = geo_clean\n",
    "print(f\"‚úÖ Geolocation duplicates removed. New Shape: {dfs['geolocation'].shape}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TRANSFORM 3: Merge Product Translations & Fix Missing Categories\n",
    "# ---------------------------------------------------------\n",
    "print(\"Processing Products...\")\n",
    "\n",
    "# Fill missing category names in Portuguese with 'outros' (others) to avoid join errors\n",
    "dfs['products']['product_category_name'] = dfs['products']['product_category_name'].fillna('outros')\n",
    "\n",
    "# Merge with the translation table (Left Join ensures we don't lose products if translation is missing)\n",
    "products_merged = pd.merge(\n",
    "    dfs['products'], \n",
    "    dfs['category_translation'], \n",
    "    on='product_category_name', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# If translation is missing (NaN), fill English column with the Portuguese name (better than nothing)\n",
    "products_merged['product_category_name_english'] = products_merged['product_category_name_english'].fillna(products_merged['product_category_name'])\n",
    "\n",
    "# Update the dataframe in our dictionary\n",
    "dfs['products'] = products_merged\n",
    "\n",
    "# Drop the portuguese column now? Optional, but keeps it clean.\n",
    "# Let's keep it for now just in case, but rely on English for the DB.\n",
    "print(\"‚úÖ Products translated to English.\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FINAL CHECK\n",
    "# ---------------------------------------------------------\n",
    "print(\"=\" * 30)\n",
    "print(f\"Orders Date Type Check: {dfs['orders']['order_purchase_timestamp'].dtype}\")\n",
    "print(f\"Geolocation Unique Zips: {dfs['geolocation'].shape[0]}\")\n",
    "print(f\"Products with English Category: {dfs['products']['product_category_name_english'].notnull().sum()} / {dfs['products'].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6301e0",
   "metadata": {},
   "source": [
    "### DEEP LOGICAL INSPECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba14999b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1Ô∏è‚É£ Time Travel Errors (Delivered < Purchased): 0 rows\n",
      "\n",
      "2Ô∏è‚É£ Price Anomalies (Price <= 0): 0 rows\n",
      "\n",
      "3Ô∏è‚É£ Orphaned Order Items: 0\n",
      "4Ô∏è‚É£ Orphaned Orders (No Customer Found): 0\n",
      "\n",
      "5Ô∏è‚É£ Products without English Category: 0\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# CHECK 1: TIME TRAVEL (Business Logic)\n",
    "# ---------------------------------------------------------\n",
    "# Do we have orders delivered BEFORE they were purchased? (Impossible)\n",
    "invalid_dates = dfs['orders'][dfs['orders']['order_delivered_customer_date'] < dfs['orders']['order_purchase_timestamp']]\n",
    "print(f\"1Ô∏è‚É£ Time Travel Errors (Delivered < Purchased): {len(invalid_dates)} rows\")\n",
    "\n",
    "if len(invalid_dates) > 0:\n",
    "    print(\"   -> displaying first 3 examples:\")\n",
    "    print(invalid_dates[['order_id', 'order_purchase_timestamp', 'order_delivered_customer_date']].head(3))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK 2: PRICE ANOMALIES\n",
    "# ---------------------------------------------------------\n",
    "# Do we have items given away for free or negative prices?\n",
    "free_items = dfs['order_items'][dfs['order_items']['price'] <= 0]\n",
    "print(f\"\\n2Ô∏è‚É£ Price Anomalies (Price <= 0): {len(free_items)} rows\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK 3: REFERENTIAL INTEGRITY (Crucial for SQL Foreign Keys)\n",
    "# ---------------------------------------------------------\n",
    "# Check: Do all Order Items belong to an Order that actually exists?\n",
    "# If this fails, SQL will reject the data when we try to create Foreign Keys.\n",
    "items_orders = set(dfs['order_items']['order_id'])\n",
    "actual_orders = set(dfs['orders']['order_id'])\n",
    "orphans = items_orders - actual_orders # Items referencing an order_id that doesn't exist in the orders table\n",
    "\n",
    "print(f\"\\n3Ô∏è‚É£ Orphaned Order Items: {len(orphans)}\")\n",
    "if len(orphans) > 0:\n",
    "    print(\"   ‚ö†Ô∏è CRITICAL: We have items for orders that don't exist in the Orders table.\")\n",
    "\n",
    "# Check: Do all Orders belong to a Customer that actually exists?\n",
    "order_custs = set(dfs['orders']['customer_id'])\n",
    "actual_custs = set(dfs['customers']['customer_id'])\n",
    "orphan_custs = order_custs - actual_custs\n",
    "\n",
    "print(f\"4Ô∏è‚É£ Orphaned Orders (No Customer Found): {len(orphan_custs)}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CHECK 4: PRODUCT CATEGORY COMPLETENESS\n",
    "# ---------------------------------------------------------\n",
    "# Did our previous merge actually work for everything?\n",
    "missing_trans = dfs['products']['product_category_name_english'].isnull().sum()\n",
    "print(f\"\\n5Ô∏è‚É£ Products without English Category: {missing_trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824cd6a3",
   "metadata": {},
   "source": [
    "### EXPORT TO LOCAL DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae318b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "üìÇ EXPORTING CLEAN DATA TO:\n",
      "D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\processed\n",
      "==============================\n",
      "‚úÖ Saved: customers.csv (99441 rows)\n",
      "‚úÖ Saved: geolocation.csv (19015 rows)\n",
      "‚úÖ Saved: orders.csv (99441 rows)\n",
      "‚úÖ Saved: order_items.csv (112650 rows)\n",
      "‚úÖ Saved: order_payments.csv (103886 rows)\n",
      "‚úÖ Saved: order_reviews.csv (99224 rows)\n",
      "‚úÖ Saved: products.csv (32951 rows)\n",
      "‚úÖ Saved: sellers.csv (3095 rows)\n",
      "‚úÖ Saved: category_translation.csv (71 rows)\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the export folder path\n",
    "export_path = r\"D:\\Data_Science\\Projects\\Portfolio_Projects\\Olist-Logistics-Analysis\\data\\processed\"\n",
    "\n",
    "# 2. Create the folder if it doesn't exist (Safety check)\n",
    "os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"üìÇ EXPORTING CLEAN DATA TO:\\n{export_path}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# 3. Iterate through the dictionary and save each dataframe\n",
    "for name, df in dfs.items():\n",
    "    # Create a filename, e.g., 'orders.csv'\n",
    "    # We use the original key name 'orders', 'products', etc.\n",
    "    filename = f\"{name}.csv\"\n",
    "    full_path = os.path.join(export_path, filename)\n",
    "    \n",
    "    # Export to CSV\n",
    "    # index=False prevents pandas from adding that annoying 0,1,2,3... column\n",
    "    df.to_csv(full_path, index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Saved: {filename} ({df.shape[0]} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbfda1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5349d76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
